{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08044f02",
   "metadata": {},
   "source": [
    "### Retrieve and save HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.berlin.de/rbmskzl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2263bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f83b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e731fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('berlin_rbmskzl_main_page.html', 'w') as openfile:\n",
    "    openfile.write(html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426b52b",
   "metadata": {},
   "source": [
    "### Get text of the HTML \n",
    "(same as in the [brandenburg notebook](https://github.com/dh-network/quadriga-fs-2/blob/develop/html_parsing/brandenburg-landesportal.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('berlin_rbmskzl_main_page.html') as openfile:\n",
    "    html_text = openfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a114e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/1936466/how-to-scrape-only-visible-webpage-text-with-beautifulsoup\n",
    "\n",
    "# Only get text that is visible on the website \n",
    "# Inlcudes ads, pointers to more info and so on \n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(soup):\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return [t.strip() for t in visible_texts if len(t) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca166d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_from_html(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f554e8",
   "metadata": {},
   "source": [
    "### Parse HTML (strukturierte parsen, one page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b004b8",
   "metadata": {},
   "source": [
    "#### 1. Parse the top news section of the website\n",
    "\n",
    "![berlin_de_top](berlin_de_top.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da3659",
   "metadata": {},
   "source": [
    "If we inspect the unredlying HTML code, we'll see that it is in a `<div>` element with a CSS class `'herounit-homepage herounit-homepage--default'`. Let's retrieve this div using BeautifulSoup syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb944c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv = soup.find(\"div\", {\"class\": \"herounit-homepage herounit-homepage--default\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(topdiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topdiv.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37511e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_h2titles = topdiv.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c79857",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_h2titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_texts =  topdiv.find_all('p', {\"class\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a267be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_texts = [x.text.strip() for x in topdiv_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdiv_h2titles = [x.text.strip() for x in topdiv_h2titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df720ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_data = []\n",
    "\n",
    "for i, text in enumerate(topdiv_texts):\n",
    "    new_entry = {}\n",
    "    new_entry['title'] = topdiv_h2titles[i]\n",
    "    new_entry['text'] = text\n",
    "    resulting_data.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912346be",
   "metadata": {},
   "source": [
    "### Parse HTML for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc72051",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feaf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852ca72",
   "metadata": {},
   "source": [
    "#### Separate internal and external links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f65b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links = []\n",
    "external_links = []\n",
    "\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    if link_url.startswith('/'):\n",
    "        internal_links.append(link_url)\n",
    "    else:\n",
    "        external_links.append(link_url)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34972e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0868003",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d28cf",
   "metadata": {},
   "source": [
    "#### parsing all internal links and getting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df069ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_data = []\n",
    "\n",
    "for link in internal_links:\n",
    "    data = {}\n",
    "    full_url = f'https://www.berlin.de/{link}'\n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code == 200:\n",
    "        html_text = response.text\n",
    "        soup = BeautifulSoup(html_text)\n",
    "        text = text_from_html(soup)\n",
    "        data['url'] = full_url\n",
    "        data['text'] = text\n",
    "        all_links_data.append(data)\n",
    "    else:\n",
    "        print(f'error {response.status_code}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ba00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ea438",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18194f",
   "metadata": {},
   "source": [
    "### Save as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a278327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf27b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{date}_berlin-senatskanzlei.json', 'w') as json_out:\n",
    "    json.dump(all_links_data, json_out, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffcab0",
   "metadata": {},
   "source": [
    "### Creating a diachronic corpus of press-releases -- ⚠️ Work in Progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a4eb2",
   "metadata": {},
   "source": [
    "Let us examine the page with press releases: https://www.berlin.de/presse/pressemitteilungen\n",
    "Iа we look at the menu, we'll see that it contains the press-releases of the Berlin authorities since 1993... The links to the pages with press-releases are quite predictable:  \n",
    "\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/2\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/3\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/4\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/5\n",
    "* ...\n",
    "* ...\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/7193\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/7194\n",
    "* https://www.berlin.de/presse/pressemitteilungen/index/index/page/7195\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a555ac",
   "metadata": {},
   "source": [
    "<s>So we could easily go over all of them and save the html files</s>  This does not work from python, apparently the server detects crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ab14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "for i in range(1, 7196):\n",
    "    url = f'https://www.berlin.de/presse/pressemitteilungen/index/index/page/{i}'\n",
    "    response = requests.get(full_url, headers=headers)\n",
    "    time.sleep(1+random.random())\n",
    "    with open(f'html_files/{i}.html', 'w') as outfile:\n",
    "        outfile.write(response.text)\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cb331",
   "metadata": {},
   "source": [
    "switching to selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8808bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409823d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url_sel(url, links, filetowrite):\n",
    "    driver.get(url)\n",
    "    # Find all the anchor tags\n",
    "    anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "    # Extract and store the URLs from the href attribute\n",
    "    for anchor in anchors:\n",
    "        href = anchor.get_attribute(\"href\")\n",
    "        if href:  # Check if href is not None\n",
    "            if 'pressemitteilung.' in href: # Check that the link looks like a press-release\n",
    "                links.append(href)\n",
    "                filetowrite.write(f'{href}\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6ffc9",
   "metadata": {},
   "source": [
    "#### parsing for links to press-releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6179806",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cec786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('presslinks.txt', 'a') as linksfile:\n",
    "    for i in tqdm(range(1215, 7196)):\n",
    "        time.sleep(random.random())\n",
    "        url = f'https://www.berlin.de/presse/pressemitteilungen/index/index/page/{i}'\n",
    "        parse_url_sel(url, all_links, linksfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e65246",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('presslinks.txt') as presslinks:\n",
    "    lines = presslinks.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [x.strip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1cff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d426b",
   "metadata": {},
   "source": [
    "#### getting texts of press-releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7dc1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for link in tqdm(lines):\n",
    "    if link not in urls:\n",
    "        this_release = {}\n",
    "        #time.sleep(random.random())\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            this_release['url'] = link\n",
    "            this_release['html'] = html\n",
    "            data.append(this_release)\n",
    "        else:\n",
    "            print(f'error {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdd896",
   "metadata": {},
   "source": [
    "### Dump it to JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'all_press_releases.json', 'w') as json_out:\n",
    "    json.dump(data, json_out, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqie_urls = set()\n",
    "unique_data = []\n",
    "for i in data:\n",
    "    thisurl = i['url']\n",
    "    if thisurl not in uniqie_urls:\n",
    "        unique_data.append(i)\n",
    "        uniqie_urls.add(thisurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11768b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c994aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_texts = []\n",
    "ordered_urls = []\n",
    "for i in tqdm(unique_data):\n",
    "    soup = BeautifulSoup(i['html'])\n",
    "    pure_text = text_from_html(soup)\n",
    "    ordered_texts.append(pure_text)\n",
    "    ordered_urls.append(i['url'])\n",
    "    #i['text'] = pure_text\n",
    "    #del i['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43db5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(ordered_texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_texts_str = ['\\n'.join(x) for x in ordered_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddabec02",
   "metadata": {},
   "source": [
    "### process with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af10d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef84135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d12698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = pd.Series(ordered_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = pd.Series(ordered_texts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8abb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(whole_text):\n",
    "    '''looks for date in raw text in format \\d\\d\\.\\d\\d\\.\\d\\d\\d\\d\n",
    "    returns in w3c format if found\n",
    "    '''\n",
    "    datesearch = re.search(r'(\\d\\d)\\.(\\d\\d)\\.((199|200|201|202)\\d)', whole_text) \n",
    "    if datesearch is not None:\n",
    "        return f'{datesearch.group(3)}-{datesearch.group(2)}-{datesearch.group(1)}' \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36599aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_date(df['text'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ded484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['text'].apply(get_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ddf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].apply(lambda x: x[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584490d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7092a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(some_text):\n",
    "    doc = nlp(some_text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f95158",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize('Haben Sie Anmerkungen oder Fragen zur Barrierefreiheit dieser Webseite?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963989ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textlem'] = df['text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(some_lemmas, some_keyword):\n",
    "    some_keyword = some_keyword.lower()\n",
    "    counter = 0\n",
    "    for lemma in some_lemmas:\n",
    "        if lemma.lower() == some_keyword:\n",
    "            counter+=1\n",
    "    return counter        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word(['aaa','aaa','aa', 'aa', 'aa', 'aa', 'a'], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ukr_freq'] = df['textlem'].apply(count_word, some_keyword='Ukraine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1996a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month').sum()['virus_freq'].plot(figsize=(10,8), title='Virus')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby('month').sum()['virus_freq']/df.groupby('month').count()['url']).plot(figsize=(10,8), title='Virus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month').sum()['ukr_freq'].plot(figsize=(10,8), title='Ukraine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month').sum()['migrant_freq'].plot(figsize=(10,8), title='Migrant')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('berlin_parsed.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe1341",
   "metadata": {},
   "source": [
    "### Older stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'all_press_releases.json', 'w') as json_out:\n",
    "    json.dump(unique_data, json_out, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = set()\n",
    "for i in data:\n",
    "    urls.add(i['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c182db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd113a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data[1]['html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_from_html(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "with open('testfile.txt', 'w') as linksfile:\n",
    "    parse_url_sel(\"https://www.berlin.de/presse/pressemitteilungen/index/index/page/2\", test_list, linksfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab15af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.berlin.de/presse/pressemitteilungen/index/index/page/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the anchor tags\n",
    "anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "# Extract and store the URLs from the href attribute\n",
    "links = []\n",
    "for anchor in anchors:\n",
    "    href = anchor.get_attribute(\"href\")\n",
    "    if href:  # Check if href is not None\n",
    "        if 'pressemitteilung.' in href: # Check that the link looks like a press-release\n",
    "            links.append(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557b132",
   "metadata": {},
   "source": [
    "*To be continued*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
